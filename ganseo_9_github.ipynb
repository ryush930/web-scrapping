{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8e9ae-c822-494e-b355-f4f6b43f6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 옵션 설정\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# 기본 URL 설정\n",
    "url = 'https://gsc.gangseo.seoul.kr/meeting/confer/search.do#none'\n",
    "driver.get(url)\n",
    "\n",
    "# 검색어 입력\n",
    "def input_search(keyword):\n",
    "    search_input = driver.find_element(By.XPATH, '//*[@id=\"keyword\"]')\n",
    "    search_input.send_keys(keyword)\n",
    "\n",
    "# 검색 버튼 클릭\n",
    "def click_search_button():\n",
    "    search_btn = driver.find_element(By.XPATH, '//*[@id=\"btnSearch\"]')\n",
    "    search_btn.click()\n",
    "\n",
    "# 페이지 내 데이터 스크래핑 함수\n",
    "def scrape_page(i, page_num_offset=0):\n",
    "    try:\n",
    "        # 페이지 네비게이션 버튼 클릭\n",
    "        page_btn = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, f'//*[@id=\"pagingNav\"]/a[{i}]'))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", page_btn)\n",
    "        print(f\"페이지 {i-2 + page_num_offset} 이동 중...\")\n",
    "\n",
    "        # 페이지 로드 대기\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"sub_detail\"]/div[2]/table/tbody/tr'))\n",
    "        )\n",
    "        print(f\"✅ 페이지 {i-2 + page_num_offset} 로드 완료!\")\n",
    "\n",
    "        all_texts = []\n",
    "\n",
    "        # tr 요소 찾기\n",
    "        tr_elements = driver.find_elements(By.XPATH, '//*[@id=\"sub_detail\"]/div[2]/table/tbody/tr')\n",
    "        print(f\"🔎 페이지 {i-2 + page_num_offset}에서 {len(tr_elements)}개의 tr 발견\")\n",
    "\n",
    "        for tr_index, tr in enumerate(tr_elements, start=1):\n",
    "            try:\n",
    "                tr_btn = tr.find_element(By.XPATH, './td[2]/a')  # <a> 태그 찾기\n",
    "                print(f\"✅ {tr_index}번째 tr 링크 발견! 이동 중...\")\n",
    "\n",
    "                # 링크 클릭\n",
    "                if not tr_btn.is_displayed() or not tr_btn.is_enabled():\n",
    "                    print(f\"⚠️ tr {tr_index} 클릭 불가 (페이지 {i-2 + page_num_offset}) - 스킵\")\n",
    "                    continue\n",
    "\n",
    "                # 새 창으로 이동\n",
    "                before_windows = len(driver.window_handles)\n",
    "                href = tr_btn.get_attribute(\"href\")\n",
    "                onclick = tr_btn.get_attribute(\"onclick\")\n",
    "\n",
    "                if href:\n",
    "                    driver.execute_script(\"arguments[0].click();\", tr_btn)\n",
    "                elif onclick:\n",
    "                    driver.execute_script(onclick)\n",
    "\n",
    "                WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > before_windows)\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "                # 내용 가져오기\n",
    "                time.sleep(10)  # 페이지 로드 대기 시간 추가\n",
    "                spk_elements = driver.find_elements(By.XPATH, '//*[@id=\"main-content\"]//span')\n",
    "                combined_text = \" \".join([span.text for span in spk_elements if span.text.strip() != \"\"])\n",
    "                all_texts.append(combined_text)\n",
    "\n",
    "                # 창 닫기\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 오류 발생 (페이지 {i-2 + page_num_offset}, tr {tr_index}): {e}\")\n",
    "        \n",
    "        # 페이지별로 파일 저장\n",
    "        save_page_data(all_texts, i-2 + page_num_offset)\n",
    "\n",
    "        # 🔹 **이 페이지에서 `tr` 버튼이 10개 미만이면, 여기서 스크래핑을 종료!**\n",
    "        if len(tr_elements) < 10:\n",
    "            print(f\"❌ 해당 페이지 {i-2 + page_num_offset}에서 tr 버튼이 10개 미만이므로 스크래핑을 종료합니다.\")\n",
    "            return False  # 반복문 종료 신호 반환\n",
    "        return True  # 다음 페이지로 진행\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 페이지 {i-2 + page_num_offset} 오류 발생: {e}\")\n",
    "        return True  # 오류 발생 시 다음 페이지 진행 (스크래핑이 끊기지 않도록)\n",
    "\n",
    "def save_page_data(all_texts, page_num):\n",
    "    file_path = f\"git@github.com:ryush930/web-scrapping.git/page{page_num}.txt\"  # GitHub 저장소 경로로 변경\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"\\n\\n\".join(all_texts))  \n",
    "    print(f\"✅ 페이지 {page_num}의 데이터가 {file_path}에 저장되었습니다.\")\n",
    "\n",
    "    # GitHub에 파일 추가 및 커밋, 푸시\n",
    "    commit_and_push(file_path, page_num)\n",
    "\n",
    "def commit_and_push(file_path, page_num):\n",
    "    try:\n",
    "        # Git 명령어 실행\n",
    "        subprocess.run(['git', 'add', file_path], check=True)\n",
    "        subprocess.run(['git', 'commit', '-m', f'페이지 {page_num} 데이터 추가'], check=True)\n",
    "        subprocess.run(['git', 'push'], check=True)\n",
    "        print(f\"✅ 페이지 {page_num}의 데이터가 GitHub에 업로드되었습니다.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ GitHub 업로드 중 오류 발생: {e}\")\n",
    "\n",
    "# 실행 부분\n",
    "input_search(\"행정사무감사\")  # 검색어 입력\n",
    "click_search_button()  # 검색 버튼 클릭\n",
    "\n",
    "# 페이지 1~10까지 네비게이션 실행\n",
    "for i in range(3, 13):  # 페이지 3번부터 12번까지\n",
    "    should_continue = scrape_page(i)\n",
    "    if not should_continue:  # `tr` 버튼이 10개 미만이면 스크래핑 종료\n",
    "        driver.quit()\n",
    "        exit()\n",
    "\n",
    "# 페이지 13 클릭하여 이후 페이지를 계속 스크래핑 (페이지 번호가 더 있을 때까지 반복)\n",
    "try:\n",
    "    page_num_offset = 10  # 페이지 번호 오프셋 (11~20부터 저장)\n",
    "    while True:\n",
    "        # 페이지 13 버튼 클릭\n",
    "        page_13_btn = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"pagingNav\"]/a[13]'))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", page_13_btn)\n",
    "        print(f\"✅ 페이지 {page_num_offset+1} 이동 중...\")\n",
    "\n",
    "        # 페이지 3부터 시작 (실제로는 page 11부터 시작)\n",
    "        for i in range(3, 13):\n",
    "            should_continue = scrape_page(i, page_num_offset)\n",
    "            if not should_continue:  # `tr` 버튼이 10개 미만이면 스크래핑 종료\n",
    "                driver.quit()\n",
    "                exit()\n",
    "\n",
    "        page_num_offset += 10  # 다음 페이지 그룹으로 이동\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 페이지 13 클릭 또는 이후 페이지 스크래핑 중 오류 발생: {e}\")\n",
    "\n",
    "# 스크래핑 완료 후 드라이버 종료\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
